{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784d3e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 16:26:24.172767: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-23 16:26:24.306477: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-23 16:26:24.308400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 16:26:27.147322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658cfe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables \n",
    "main_folder = '/mnt/c/users/efazh/DataCapstone/'\n",
    "images_folder = main_folder + 'img_align_celeba/img_align_celeba/'\n",
    "\n",
    "TRAINING_SAMPLES = 1000\n",
    "VALIDATION_SAMPLES = 500\n",
    "BATCH_SIZE = 10\n",
    "TEST_SAMPLES = 2000\n",
    "IMG_WIDTH = 178\n",
    "IMG_HEIGHT = 218"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c1e05",
   "metadata": {},
   "source": [
    "# Choose a Pre-trained Model (Transfer Learning)\n",
    "\n",
    "We will use a pre-trained model such as VGG16 that was trained on the ImageNet dataset. Transfer learning allows us to leverage the knowledge from these pre-trained models and adapt it to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "305977d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ImageDataGenerator for training with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,           # Normalize pixel values to [0, 1]\n",
    "    horizontal_flip=True,      # Random horizontal flip\n",
    "    rotation_range=30,         # Random rotation\n",
    "    brightness_range=[0.8, 1.2], # Random brightness adjustment\n",
    "    zoom_range=0.2,            # Random zooming\n",
    "    width_shift_range=0.2,     # Random width shift\n",
    "    height_shift_range=0.2,    # Random height shift\n",
    "    validation_split=0.2       # Split off 20% of data for validation\n",
    ")\n",
    "\n",
    "# ImageDataGenerator for validation without augmentation (only rescaling)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# ImageDataGenerator for the test set (only rescaling)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Define the folder where your images are stored\n",
    "image_directory = images_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    image_directory,            # Path to image directory\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),  # Resize all images to uniform size\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',   # Use 'categorical' for multi-class or multi-label tasks\n",
    "    subset='training'           # Use the training subset\n",
    ")\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    image_directory,            # Path to image directory\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),  # Resize all images\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',   # Use 'categorical' for multi-class or multi-label tasks\n",
    "    subset='validation'         # Use the validation subset\n",
    ")\n",
    "\n",
    "# Test data generator (test set can be from a separate folder)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    image_directory,            # Path to image directory\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),  # Resize all images\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',   # Use 'categorical' for multi-class or multi-label tasks\n",
    "    shuffle=False               # Do not shuffle the test data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625c7d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 16:29:47.361344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-23 16:29:47.362356: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load the VGG16 model with pre-trained weights, without the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Freeze the layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe008b01",
   "metadata": {},
   "source": [
    "# Set Up Input Layers and Preprocessing\n",
    "\n",
    "We freeze the pre-trained layers to retain the ImageNet weights and add custom layers for our specific task. This will allow the base model to extract important features from the input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd21a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# Output layer (adjust the number of output classes based on the task)\n",
    "output_layer = Dense(40, activation='sigmoid')(x)  # 40 attributes for multi-label classification\n",
    "\n",
    "# Define the final model\n",
    "from tensorflow.keras.models import Model\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d11db",
   "metadata": {},
   "source": [
    "# Compiling the Model\n",
    "\n",
    "After defining the architecture, compile the model using the Adam optimizer and the binary cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae21054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 218, 178, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 218, 178, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 218, 178, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 109, 89, 64)       0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 109, 89, 128)      73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 109, 89, 128)      147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 54, 44, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 54, 44, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 54, 44, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 54, 44, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 27, 22, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 27, 22, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 27, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 27, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 13, 11, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 13, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 13, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 13, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 6, 5, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 40)                20520     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,785,320\n",
      "Trainable params: 1,070,632\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Show the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21e0b83",
   "metadata": {},
   "source": [
    "# Training Model\n",
    "\n",
    "Now, we train the model using the augmented data from the ImageDataGenerator. We will monitor the validation accuracy during training and adjust the model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=TRAINING_SAMPLES // BATCH_SIZE,\n",
    "    validation_steps=VALIDATION_SAMPLES // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7d8bce",
   "metadata": {},
   "source": [
    "# Evaluating Model\n",
    "\n",
    "After training, evaluate the model's performance using the test set to measure its generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=TEST_SAMPLES // BATCH_SIZE)\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd4281",
   "metadata": {},
   "source": [
    "# Visualize Training Process\n",
    "\n",
    "To better understand the training process, let's plot the training and validation accuracy and loss over the epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy over epochs\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('facial_recognition_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
